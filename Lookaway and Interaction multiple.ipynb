{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4078f83-fd0c-4c18-b4c2-7d9c8a0d4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial head yaw for person 1: -22.55 degrees\n",
      "Initial head yaw for person 2: -6.71 degrees\n",
      "Initial head yaw for person 3: -41.58 degrees\n",
      "Alert: Person 1 is looking away! Look-away count: 1\n",
      "Alert: Person 2 is looking away! Look-away count: 1\n",
      "Alert: Person 3 is looking away! Look-away count: 1\n",
      "Alert: Person 1 is looking away! Look-away count: 2\n",
      "Alert: Person 2 is looking away! Look-away count: 2\n",
      "Alert: Person 2 is looking away! Look-away count: 3\n",
      "Alert: Person 2 is looking away! Look-away count: 4\n",
      "Alert: Person 1 is looking away! Look-away count: 3\n",
      "Alert: Person 1 is looking away! Look-away count: 4\n",
      "Alert: Person 2 is looking away! Look-away count: 5\n",
      "Alert: Person 2 is looking away! Look-away count: 6\n",
      "Alert: Person 2 is looking away! Look-away count: 7\n",
      "Alert: Person 2 is looking away! Look-away count: 8\n",
      "Alert: Person 2 is looking away! Look-away count: 9\n",
      "Alert: Person 2 is looking away! Look-away count: 10\n",
      "Alert: Person 2 is looking away! Look-away count: 11\n",
      "Alert: Person 2 is looking away! Look-away count: 12\n",
      "Initial head yaw for person 4: -13.63 degrees\n",
      "Alert: Person 3 is looking away! Look-away count: 2\n",
      "Alert: Person 2 is looking away! Look-away count: 13\n",
      "Alert: Person 3 is looking away! Look-away count: 3\n",
      "Alert: Person 2 is looking away! Look-away count: 14\n",
      "Alert: Person 2 is looking away! Look-away count: 15\n",
      "Alert: Person 1 is looking away! Look-away count: 5\n",
      "Alert: Person 2 is looking away! Look-away count: 16\n",
      "Alert: Person 2 is looking away! Look-away count: 17\n",
      "Alert: Person 2 is looking away! Look-away count: 18\n",
      "Alert: Person 2 is looking away! Look-away count: 19\n",
      "Alert: Person 2 is looking away! Look-away count: 20\n",
      "Alert: Person 2 is looking away! Look-away count: 21\n",
      "Alert: Person 2 is looking away! Look-away count: 22\n",
      "Alert: Person 1 is looking away! Look-away count: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shriv\\AppData\\Local\\Temp\\ipykernel_18216\\2320837607.py:64: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sy = np.sqrt(rotation_matrix[0, 0]*2 + rotation_matrix[1, 0]*2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: Person 1 is looking away! Look-away count: 7\n",
      "Alert: Person 3 is looking away! Look-away count: 4\n",
      "Alert: Person 1 is looking away! Look-away count: 8\n",
      "Alert: Person 2 is looking away! Look-away count: 23\n",
      "Alert: Person 1 is looking away! Look-away count: 9\n",
      "Alert: Person 2 is looking away! Look-away count: 24\n",
      "Alert: Person 1 is looking away! Look-away count: 10\n",
      "Alert: Person 2 is looking away! Look-away count: 25\n",
      "Alert: Person 2 is looking away! Look-away count: 26\n",
      "Alert: Person 3 is looking away! Look-away count: 5\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe face mesh and drawing\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Define landmarks for left, right eye, and chin\n",
    "LEFT_EYE = [33, 133]\n",
    "RIGHT_EYE = [362, 263]\n",
    "CHIN = 152\n",
    "NOSE_TIP = 1\n",
    "\n",
    "# Global variables to store the initial head pose for up to five faces\n",
    "initial_yaws = [None] * 5  # List to store the initial yaw for up to five faces\n",
    "look_away_counters = [0] * 5  # Counter for each person\n",
    "look_away_flags = [False] * 5  # Flag to track ongoing look-away state\n",
    "\n",
    "# Function to get 3D coordinates of landmarks\n",
    "def get_3d_landmark(face_landmarks, idx, image_shape):\n",
    "    h, w, _ = image_shape\n",
    "    landmark = face_landmarks.landmark[idx]\n",
    "    return np.array([landmark.x * w, landmark.y * h, landmark.z * w])  # z is scaled by width\n",
    "\n",
    "# Function to calculate the head pose using nose, chin, and eyes\n",
    "def get_head_pose(left_eye_3d, right_eye_3d, nose_3d, chin_3d, image_shape):\n",
    "    model_points = np.array([\n",
    "        (0.0, 0.0, 0.0),         # Nose tip\n",
    "        (0.0, -100.0, 0.0),      # Chin\n",
    "        (-50.0, 50.0, 0.0),      # Left eye\n",
    "        (50.0, 50.0, 0.0)        # Right eye\n",
    "    ])\n",
    "\n",
    "    # 2D image points from face landmarks\n",
    "    image_points = np.array([\n",
    "        nose_3d[:2],  # Nose tip\n",
    "        chin_3d[:2],  # Chin\n",
    "        left_eye_3d[:2],  # Left eye\n",
    "        right_eye_3d[:2]  # Right eye\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "    # Camera internals (using image shape)\n",
    "    focal_length = image_shape[1]\n",
    "    center = (image_shape[1] / 2, image_shape[0] / 2)\n",
    "    camera_matrix = np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "    # Assuming no lens distortion\n",
    "    dist_coeffs = np.zeros((4, 1))\n",
    "\n",
    "    # Solve PnP to get rotation and translation vectors\n",
    "    success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "        model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "\n",
    "    return rotation_vector, translation_vector\n",
    "\n",
    "# Function to convert rotation vector to Euler angles (yaw, pitch, roll)\n",
    "def rotation_vector_to_euler_angles(rotation_vector):\n",
    "    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "    sy = np.sqrt(rotation_matrix[0, 0]*2 + rotation_matrix[1, 0]*2)\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if not singular:\n",
    "        x = np.arctan2(rotation_matrix[2, 1], rotation_matrix[2, 2])\n",
    "        y = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "        z = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
    "    else:\n",
    "        x = np.arctan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])\n",
    "        y = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.degrees(x), np.degrees(y), np.degrees(z)  # Pitch, yaw, roll\n",
    "\n",
    "# Function to check if the user is looking away based on initial head pose\n",
    "def is_looking_away_relative(current_yaw, initial_yaw, gaze_threshold=20):\n",
    "    yaw_difference = current_yaw - initial_yaw\n",
    "    return abs(yaw_difference) > gaze_threshold\n",
    "\n",
    "# Function to check if two gaze lines intersect\n",
    "def do_gaze_lines_intersect(eye1_center, eye2_center, gaze1_dir, gaze2_dir):\n",
    "    distance_between_eyes = np.linalg.norm(eye1_center - eye2_center)\n",
    "    angle_diff = np.abs(np.dot(gaze1_dir, gaze2_dir))\n",
    "    return distance_between_eyes < 1000 and angle_diff < 0.8  # Adjusted thresholds\n",
    "\n",
    "# Initialize webcam and face mesh detector\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # Set width\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # Set height\n",
    "\n",
    "# Thresholds for gaze direction alerts\n",
    "GAZE_AWAY_THRESHOLD = 20  # Base degrees threshold for yaw angle\n",
    "\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, max_num_faces=5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip image and convert to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = face_mesh.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        eye_centers = []\n",
    "        gaze_directions = []\n",
    "        face_rects = []\n",
    "\n",
    "        # Process faces detected in frame\n",
    "        if results.multi_face_landmarks:\n",
    "            for idx, face_landmarks in enumerate(results.multi_face_landmarks):\n",
    "                if idx >= 5:  # Limit to a maximum of five faces\n",
    "                    break\n",
    "                \n",
    "                # Get 3D coordinates for landmarks\n",
    "                left_eye_3d = get_3d_landmark(face_landmarks, LEFT_EYE[0], image.shape)\n",
    "                right_eye_3d = get_3d_landmark(face_landmarks, RIGHT_EYE[0], image.shape)\n",
    "                nose_3d = get_3d_landmark(face_landmarks, NOSE_TIP, image.shape)\n",
    "                chin_3d = get_3d_landmark(face_landmarks, CHIN, image.shape)\n",
    "\n",
    "                # Calculate head pose (rotation vector)\n",
    "                rotation_vector, translation_vector = get_head_pose(left_eye_3d, right_eye_3d, nose_3d, chin_3d, image.shape)\n",
    "\n",
    "                # Convert rotation vector to Euler angles (yaw, pitch, roll)\n",
    "                pitch, yaw, roll = rotation_vector_to_euler_angles(rotation_vector)\n",
    "\n",
    "                # Capture the initial yaw for each face if it's not set yet\n",
    "                if initial_yaws[idx] is None:\n",
    "                    initial_yaws[idx] = yaw\n",
    "                    print(f\"Initial head yaw for person {idx+1}: {initial_yaws[idx]:.2f} degrees\")\n",
    "\n",
    "                # Check if the person is looking away based on yaw difference from initial yaw\n",
    "                is_looking_away = is_looking_away_relative(yaw, initial_yaws[idx], GAZE_AWAY_THRESHOLD)\n",
    "\n",
    "                # Only increment look-away counter once per look-away event\n",
    "                if is_looking_away:\n",
    "                    if not look_away_flags[idx]:  # If not already flagged as looking away\n",
    "                        look_away_counters[idx] += 1  # Increment look-away counter\n",
    "                        look_away_flags[idx] = True   # Set the look-away flag\n",
    "                        print(f\"Alert: Person {idx+1} is looking away! Look-away count: {look_away_counters[idx]}\")\n",
    "                else:\n",
    "                    look_away_flags[idx] = False  # Reset the flag when the person is not looking away\n",
    "\n",
    "                # Draw gaze direction line\n",
    "                gaze_direction = (right_eye_3d[:2] - left_eye_3d[:2]) / np.linalg.norm(right_eye_3d[:2] - left_eye_3d[:2])\n",
    "                eye_center = np.mean([left_eye_3d[:2], right_eye_3d[:2]], axis=0)\n",
    "                eye_centers.append(eye_center)\n",
    "                gaze_directions.append(gaze_direction)\n",
    "                end_point = (int(eye_center[0] + gaze_direction[0] * 100), int(eye_center[1] + gaze_direction[1] * 100))\n",
    "                cv2.line(image, tuple(eye_center.astype(int)), end_point, (0, 255, 0), 2)\n",
    "\n",
    "                # Draw bounding box if the person is looking away\n",
    "                if is_looking_away:\n",
    "                    face_x_min = int(min(face_landmarks.landmark[LEFT_EYE[0]].x, face_landmarks.landmark[RIGHT_EYE[0]].x) * image.shape[1]) - 50\n",
    "                    face_x_max = int(max(face_landmarks.landmark[LEFT_EYE[0]].x, face_landmarks.landmark[RIGHT_EYE[0]].x) * image.shape[1]) + 50\n",
    "                    face_y_min = int(min(face_landmarks.landmark[LEFT_EYE[0]].y, face_landmarks.landmark[CHIN].y) * image.shape[0]) - 50\n",
    "                    face_y_max = int(max(face_landmarks.landmark[LEFT_EYE[0]].y, face_landmarks.landmark[CHIN].y) * image.shape[0]) + 50\n",
    "                    cv2.rectangle(image, (face_x_min, face_y_min), (face_x_max, face_y_max), (0, 0, 255), 2)\n",
    "\n",
    "                # Draw face mesh\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    mp_drawing.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1),\n",
    "                    mp_drawing.DrawingSpec(color=(80, 256, 121), thickness=1, circle_radius=1)\n",
    "                )\n",
    "\n",
    "                # Display the look-away count on the screen for each person\n",
    "                cv2.putText(image, f'Look-aways: {look_away_counters[idx]}',\n",
    "                            (50, 130 + idx * 30),  # Adjust the position as needed\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Check for gaze interactions between people\n",
    "            for i in range(len(eye_centers)):\n",
    "                for j in range(i + 1, len(eye_centers)):\n",
    "                    if do_gaze_lines_intersect(eye_centers[i], eye_centers[j], gaze_directions[i], gaze_directions[j]):\n",
    "                        cv2.putText(image, \"Gaze Interaction Detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Gaze Tracking with Interaction Detection\", image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f4f03-30de-43da-8316-a9d49ef994cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8561af1-4428-4b88-bb70-adc9002b7488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSP",
   "language": "python",
   "name": "csp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
